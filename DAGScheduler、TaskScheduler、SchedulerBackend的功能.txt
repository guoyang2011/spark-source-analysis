---
date: 2016-05-03 13:50
status: public
title: DAGScheduler、TaskScheduler、SchedulerBackend的功能
---

###DAGScheduler 功能
The high-level scheduling layer that implements stage-oriented scheduling. It computes a DAG of stages for each job, keeps track of which RDDs and stage outputs are materialized, and finds a minimal schedule to run the job. It then submits stages as TaskSets to an underlying TaskScheduler implementation that runs them on the cluster. A TaskSet contains fully independent tasks that can run right away based on the data that's already on the cluster (e.g. map output files from previous stages), though it may fail if this data becomes unavailable.
Spark stages are created by breaking the RDD graph at shuffle boundaries. RDD operations with "narrow" dependencies, like map() and filter(), are pipelined together into one set of tasks in each stage, but operations with shuffle dependencies require multiple stages (one to write a set of map output files, and another to read those files after a barrier). In the end, every stage will have only shuffle dependencies on other stages, and may compute multiple operations inside it. The actual pipelining of these operations happens in the RDD.compute() functions of various RDDs (MappedRDD, FilteredRDD, etc).
In addition to coming up with a DAG of stages, the DAGScheduler also determines the preferred locations to run each task on, based on the current cache status, and passes these to the low-level TaskScheduler. Furthermore, it handles failures due to shuffle output files being lost, in which case old stages may need to be resubmitted. Failures within a stage that are not caused by shuffle file loss are handled by the TaskScheduler, which will retry each task a small number of times before cancelling the whole stage.
When looking through this code, there are several key concepts: 
   - Jobs (represented by **ActiveJob**) are the top-level work items submitted to the scheduler.
     For example, when the user calls an action, like count(), a job will be submitted through ***submitJob***. Each Job may require the execution of multiple stages to build intermediate data.
 
   - Stages (**Stage**) are sets of tasks that compute intermediate results in jobs, where each task computes the same function on partitions of the same RDD. Stages are separated at shuffle boundaries, which introduce a barrier (where we must wait for the previous stage to finish to fetch outputs). There are two types of stages: ***ResultStage***, for the final stage that executes an action, and ***ShuffleMapStage***, which writes map output files for a shuffle.
     Stages are often shared across multiple jobs, if these jobs reuse the same RDDs.
 
   - Tasks are individual units of work, each sent to one machine.
 
   - Cache tracking: the DAGScheduler figures out which RDDs are cached to avoid recomputing them and likewise remembers which shuffle map stages have already produced output files to avoid redoing the map side of a shuffle.
 
   - Preferred locations: the DAGScheduler also computes where to run each task in a stage based on the preferred locations of its underlying RDDs, or the location of cached or shuffle data.
 
   - Cleanup: all data structures are cleared when the running jobs that depend on them finish,to prevent memory leaks in a long-running application.
To recover from failures, the same stage might need to run multiple times, which are called "attempts". If the TaskScheduler reports that a task failed because a map output file from a previous stage was lost, the DAGScheduler resubmits that lost stage. This is detected through a CompletionEvent with FetchFailed, or an ExecutorLost event. The DAGScheduler will wait a small amount of time to see whether other nodes or tasks fail, then resubmit TaskSets for any lost stage(s) that compute the missing tasks. As part of this process, we might also have to create Stage objects for old (finished) stages where we previously cleaned up the Stage object. Since tasks from the old attempt of a stage could still be running, care must be taken to map any events received in the correct Stage object.
Here's a checklist to use when making or reviewing changes to this class:
1. All data structures should be cleared when the jobs involving them end to avoid indefinite accumulation of state in long-running programs.
2. When adding a new data structure, update `DAGSchedulerSuite.assertDataStructuresEmpty` to include the new structure. This will help to catch memory leaks.


------
###TaskScheduler功能
Low-level task scheduler interface, currently implemented exclusively by [[org.apache.spark.scheduler.TaskSchedulerImpl]].This interface allows plugging in different task schedulers. Each TaskScheduler schedules tasks for a single SparkContext. These schedulers get sets of tasks submitted to them from the DAGScheduler for each stage, and are responsible for sending the tasks to the cluster, running them, retrying if there are failures, and mitigating stragglers. They return events to the DAGScheduler.

------
###SchedulerBackend功能
A backend interface for scheduling systems that allows plugging in different ones under TaskSchedulerImpl. We assume a Mesos-like model where the application gets resource offers as machines become available and can launch tasks on them.
 ------
 
1.DAGScheduler是spark中面向stage的上层调度器，负责将一个job中按照不同RDD依赖关系拆分成多个stage,并将每个stage中生成的TaskSets提交TaskScheduler底层调度器
2.TaskScheduler是spark中底层的面向Task的调度器，负责提交Task到集群中执行和监控Task的运行状态，并向DAGScheduler汇报Task的运行状态。Spark中默认实现为TaskSchedulerImpl
3.TaskScheduler是可以实现不同调度策略的一个插件，Spark中使用ScheudlerBackend的抽象接口来实现支持不同的调度系统(Mesos,Yarn,Spark-standalone等)，在SparkContext初始化的时候(***SparkContext.createTaskScheduler(...)***)会根据SparkConf中***spark.master***属性创建相应的SchedulerBackend。
4.按照dependency不同,stage分为两类ShuffleMapStage和ResultStage,相应的Task也存在两种类型：ShuffleMapTask和

>##Shuffle
>

Spark stages are created by breaking the RDD graph at shuffle boundaries. RDD operations with "narrow" dependencies, like map() and filter(), are pipelined together into one set of tasks in each stage, but operations with shuffle dependencies require multiple stages (one to write a set of map output files, and another to read those files after a barrier). In the end, every stage will have only shuffle dependencies on other stages, and may compute multiple operations inside it. The actual pipelining of these operations happens in the RDD.compute() functions of various RDDs (MappedRDD, FilteredRDD, etc).

Shuffle过程分为两个阶段：
1.Mapper端按照Partitioner算法和partition数量将计算结果写入到本地文件系统，写入文件数等于partition数量，一个文件对应一个分片数据。
2.Reducer端每个partition按照partitionId读取mapper端本地文件系统的分片数据。

------
>作者:[快鸟2016](https://fastbird2016.farbox.com)
邮箱:<fastbird2016@gmail.com>
更新时间：2016年04月14日
>